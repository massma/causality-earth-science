\documentclass[12pt]{article}
\input{def}
\graphicspath{{figs/}}

\begin{document}

% \title{Causality for clarification: examples from Earth science}
\title{Causality for the Earth sciences}

\author{Adam Massmann\thanks{Corresponding author:
    akm2203@columbia.edu}, Pierre Gentine, Jakob Runge, Elias Bareinboim}

\maketitle
\begin{abstract}
  The use of causal methods is exploding in the Earth
  sciences. However, most applications have focused on causal
  discovery, i.e. inferring the causal relationships and structure
  from data. This paper looks at causality through the lens of causal
  inference and examines how defined causal graphs, a fundamental from
  causal theory, can be used to clarify assumptions, identify
  tractable problems, and aid interpretation of results in Earth
  science research. We apply causal theory to generic graphs of the
  Earth system to identify where causal inference may be most
  tractable and useful to address problems in the Earth science and
  avoid potentially statistically incorrect conclusions. Specifically,
  causal inference may be particularly powerful when: (1) the state
  space of the system is reconstructable from lagged observations of
  the system; or: (2) the effect of interest is only causally affected
  by the observed portion of the state space; or: (3) The cause of
  interest can be assumed to be independent of the evolution of the
  system’s state. However, we also highlight through examples that
  inclusion of a causal graph in any analysis provides explicit
  definition and communication of assumptions and helps to structure
  analyses, even if causal inference is ultimately challenging from
  the data available.
\end{abstract}

\section{Introduction}

The use of causal tools is booming across the Earth sciences
\citep[e.g.,][]{ebert-uphoff2012,
  samarasinghe-casuality,runge-causal-timeseries,runge2019inferring,goodwell-causality-2020}. However,
most of this work focuses on causal discovery, or the inference (using
data) of causal structure: ``links'' and directions between
variables. This is then used to define a causal graphs and the
relationships between variables when the graph is not know a
priori. However in many applications, the causal graph is already
known based on physical insights. For instance the impact of El Niño
on West American rainfall is known to be causal and the graph does not
need to be discovered.

This paper looks at causality through a different, but complementary,
lens and examines how assumed causal graphs \citep{pearl1995causal}, a
fundamental from causal theory, can be used to clarify assumptions,
identify tractable problems, and aid interpretation of results in
Earth science research. Our goal is to distill \citep{olah2017} the
basics of the graphical approach to causality in a way that is
relatable for Earth scientists, hopefully motivating more widespread
use and adoption of causal graphs to organize analyses and communicate
assumptions. These tools are relevant now more than ever, as the
cornucopia of new data analysis tools have inevitable led to more
opaque results and barriers to the communication of assumptions.

Beyond their usefulness as communication tools, if certain conditions
are met causal graphs can be used to calculate, from data, the
generalized functional form of relationships between Earth science
variables. Ultimately, deriving generalized functional relationships
is a primary goal of science. While we know the functional
relationships between some variables a priori, there are many
relationships we do not know \citep[e.g., ecosystem scale water and
carbon fluxes;][]{massmann2019} - \textit{Grosslord et al 2020 on VPD,
  Zhou et al., 2019 PNAS, Sci Adv}, or that we do know but are
computationally intractable to calculate \citep[e.g., clouds and
microphysics at the global scale:][]{randall2003, gentine2018,
  zadra2018}. In these types of applications, causal graphs give us a
path toward new scientific knowledge and hypothesis testing:
generalized functional relationships that were inaccessible with
traditional tools.

The main contribution of this paper is to demonstrate how causal
graphs, a fundamental tool of causal inference introduced in Section
\ref{sec:what-caus-caus}, can be used to communicate assumptions,
organize analyses, and ultimately improve scientific understanding and
hypothesis testing. We want to emphasize that almost any study could
benefit from inclusion of a causal graph in terms of communication and
clarification, even if in the end the results cannot be interpreted
causally. Causal graphs also encourage us to think deeply in the
initial stages of analysis about project hypotheses testing and how
the system is structured, and can identify infeasible studies early in
the research process (before time spent on analysis, acquiring data,
building/running models, etc.). These points require some background
and discussion, so the paper is divided into the following sections:

\begin{itemize}
\item Section \ref{sec:what-caus-caus}: Introduce and discuss causal
  graphs within the general philosophy of causality and applications
  in Earth science.
\item Section \ref{sec:causal-graphs-pearls}: Using a simple relatable
  example we explain the problem of confounding and how causal graphs
  can be used to isolate the functional mapping between interventions
  on some variable(s) to their effect on other variable(s).
\item Section \ref{sec:causal-graphs-as}: We draw on a past example
  from our own research that benefits from inclusion of a causal
  graph, in terms of communicating assumptions, and organizing and
  justifying analyses.
\item Section \ref{sec:necess-cond-caus}: We turn to more generic
  examples of graphs that are generally consistent with a wide variety
  of systems in Earth science, to highlight some of the difficulties
  we confront when using causal inference in Earth science and how we
  may be able to overcome these challenges.
\end{itemize}

We hope that this paper will increase understanding of causal graphs
and encourage more use of causal fundamentals in applications. We
believe wider adoption will set us on a path towards scientific
clarity.

\section{What is causality?: the causal graph perspective and its
  usefulness in the Earth sciences}\label{sec:what-caus-caus}

We view causality through the lens of causal graphs, as introduced in
\citet{pearl1995causal}, mostly because we believe causal graphs are
useful in Earth science, rather than because of any particular
philosophical argument for causal graphs as the ``true'' definition of
causality.

Causal graphs are directed acyclic graphs (DAGs) that encode our
assumptions about the causal dependencies of a system.  To make a
causal graph, a domain expert simply draws directed edges
(i.e. arrows) from variables that are causes to effects. In other
words, to make a causal graph you simply draw a picture summarizing
the assumed causal links between variables (e.g., Figure
\ref{fig:toy}A). Causal graphs are useful tools because they can be
drawn by any domain expert with no required knowledge of maths or
probability, but they also represent formal mathematical
objects. Specifically, underlying each causal graph are a set of
equations: each node corresponds to a generating function for that
variable. The function's inputs are all of the node's parents (plus a
random variability term, which we will discuss in Section
\ref{sec:causal-graphs-pearls}). Parents are the other nodes in the
graph that point to a node of interest (e.g., in the most simple graph
$X \to Y$; $X$ is a parent of $Y$). So in reality, drawing arrows from
``causes'' to ``effects'' is synonymous with drawing arrows from
function inputs to generating functions.

In this way, drawing a causal graph is another way to visualize and
reason about a complicated system of equations, which is a very useful
for the Earth scientist: we deal with complicated interacting systems
of equations and welcome any tool that helps us understand and reason
about their collective behavior. In some cases we may know a priori
(from physics) the equations for a given function in a causal
graph. However, in practice we often either do not know all of the
functions a priori \citep[e.g., plant stomata response to
VPD;][]{massmann2019}, or some functions are computationally
intractable to compute \citep[e.g., turbulence, moist convection, and
cloud microphysics in large scale models;][]{zadra2018}. In these
scenarios the benefits of causal graphs are fully realized: based on
the causal graph we can calculate from data, using the
\textit{do-}calculus \citep{pearl-1994-do-calculus}, the functional
relationship between target variables (i.e. effects) and interventions
on any other variables in the graph (i.e. causes).

By viewing causal graphs through this pragmatic lens of calculating
the functional form of functions that we do not know a priori, we
simultaneously identify casual graphs' value for Earth scientists
while also side stepping philosophical arguments about the meaning of
causality. Causal graphs are pragmatic because in the Earth sciences
we often need to estimate how the system responds to
interventions. For example, for sub-grid parameterization we need to
estimate time tendencies' response to interventions on the large scale
state. We also may desire to calculate experiments: for example how
changing land cover from forest to grasslands affects (the statistics
of) surface temperature. \textit{Do-}calculus provides a way to
calculate this response to interventions without relying on
approximate numerical models or (potentially infeasible or unethical)
real world experimentation. While we want to maintain this emphasis on
causality as a method for calculating generalized functional forms of
responses to intervention, for consistency with the causal literature
we will call the response variables ``effects'', and the intervened
upon variables ``causes''.

For some, it may not be clear how the functional response to
interventions is different from naive regression between observed
variables. We will demonstrate in Section
\ref{sec:causal-graphs-pearls} how uninformed regression is just a
functional mapping of associations between variables, and how this
differs from the response to interventions. This is the problem that
\textit{do-}calculus solves: it identifies which data are needed and
how we can use those data to calculate the functional mapping of
interventions, rather than just associations that may be attributable
to other processes entirely. Because interventions generalize much
better than associations, \textit{do-}calculus is especially relevant
for scientists and engineers. Working through an example will clarify
some of these claims.

\section{A Toy Example: the problem of confounding and the necessity
  of \textit{do-}calculus for calculating interventions}
\label{sec:causal-graphs-pearls}
% intro causal graphs}

We demonstrate the problem of confounding and the necessity of
\textit{do-}calculus we use a very simple toy example involving
clouds, aerosols, and surface solar radiation/sunlight (Figure
\ref{fig:toy}A). Our causal graph consists of:

\begin{figure}
  % % consider \noindent\include... [width=0.75\textwidth]...
  % \includegraphics[height=0.4\textheight]{cloud-aerosol-bidirected.pdf}\\
  \scalebox{1.0}{\input{figs/cloud-aerosol.tex}}
  \caption{A toy graph example to demonstrate basic causal theory,
    involving cloud (C), aerosol (A), and surface solar radiation
    (S). In \textbf{A)} we observe all variables, while in \textbf{B)}
    we do not observe aerosol and we use the semantic of a dashed node
    to denote the unobserved variable. \textbf{C)} presents an
    alternative notation for representing an unobserved or hidden
    common cause that is popular in causal literature \citep[e.g.,
    ``semi-Markovian graphs'',][]{shpitser2006}. When using this
    notation it is common practice to only include observed processes
    as nodes in the graph. However in the Earth Science we often
    observe much less of the system relative to our knowledge of
    causal dependencies in the system, so we find it more conceptually
    clear to include unobserved common causes explicitly in the graph,
    and prefer the notation in \textbf{B)} to \textbf{C}). Any
    semi-Markovian graph can be represented as a graph with unobserved
    nodes, and vice-versa \citep[e.g.,][]{lee2019structural}.}
  \label{fig:toy}
\end{figure}

\begin{enumerate}
\item An edge from aerosols to clouds because aerosols serve as cloud
  condensation nuclei and affect the probability of water vapor
  conversion to cloud.
\item An edge from aerosols to surface solar radiation, because
  aerosols can reflect sunlight back to space and reduce sunlight at
  the surface.
\item An edge from clouds to sunlight, because clouds also reflect
  sunlight back to space and can reduce sunlight at the surface.
\end{enumerate}

Causal graphs encode our assumptions about how the system behaves, and
the nodes and edges that are \textit{missing} from the graph often
represent strong assumptions. For example, in the
cloud-aerosol-sunlight example, clouds also affect aerosols; e.g., by
increasing the likelihood that aerosol will be scavenged from the
atmosphere during precipitation \citep[e.g.,][]{radke-scavenge-1980,
  jurado2008, blanco-alegre2018}. By not including an edge from cloud
to aerosol, we are making the assumption that we are negelcting the
effect of clouds on aerosols. Considering this example is intended to
be pedagogical for introducing causal theory to the readers, we will
continue with the graph as drawn in Figure \ref{fig:toy} (Section
\ref{sec:necess-cond-caus} explores realistic Earth system graphs).

Even though mathematical reasoning is not required to construct a
causal graph, the resulting graph encodes specific mathematical
meaning based on physical understanding of the system. For example,
the graph corresponds to a set of underlying functions for each
variable:

\begin{align}
  \label{eq:2}
  aerosol &= f(U_{aerosol}) \\
  cloud &= f(aerosol, U_{cloud})\\
  sunlight &= f(aerosol, cloud, U_{sunlight})
\end{align}

where $U$ are random variables due to all the factors not represented
explicitly in the causal graph, and $f$ are deterministic functions
that generate each variable in the graph from their parents and
corresponding $U$.

The presence of the random variables $U$ introduce a third meaning to
the causal graph: they induce a specific factorization of the joint
distribution between variables into conditional and marginal factors:
\begin{equation}
  P(A, C, S) = P(S \, | \,C, A) \, P(C \, | \, A) \, P(A),
\end{equation}
where $A$ represents aerosol, $C$ represents cloud, $S$ represents
surface solar radiation, and $P(\cdot | \cdot)$ denotes conditional
probability (Appendix \ref{prob-theory} describes the notation used in
this paper and a brief introduction to probability theory for
unfamiliar readers). The inclusion of randomness in causal graphs is a
key tool: by positing a causal graph, we are not stating that the
variables in the graph are the only processes in the system. Instead,
we are stating that all other processes not included in the graph
induce variations in the graph's variables that are independent of
each other (e.g., all $U_{\cdot}$ in Equation (\ref{eq:2}) are
independent). For example, sources of aerosol variability not
considered in Figure \ref{fig:toy}A include anthropogenic aerosol
emission, the biosphere, fires, volcanoes,
etc. \citep[e.g.,][]{Boucher2015}. For cloud, this includes synoptic
forcing, atmospheric humidity,
etc. \citep[e.g.,][]{wallace2006atmospheric}. For radiation, this
includes variability of top of atmosphere radiation,
etc. \citep[e.g.,][]{hartmann2015global}. Figure \ref{fig:toy}A states
that all these external, or \textit{exogenous}, sources of variability
are independent of each other (in very technical terms, this means the
graph is ``\textit{Markovian}'').

We can now apply causal graph theory
\citep[e.g.,][]{pearl1995causal,shpitser2006} to the assumptions
encoded in our causal graph to identify which distributions must be
estimated from data in order to calculate the response of effect(s)
(e.g. of sunlight) to an experimental intervention on the cause(s)
(e.g. presence or absence of a cloud). The challenge of causal
inference is to derive the response to the intervention in terms of
only observed distributions. This process of identifying the necessary
observed distributions is formally termed \emph{causal
  identification}. If a causal effect is not identifiable
(\emph{un}-identifiable), for example if calculating a causal effect
requires distributions of variables that we do not observe, then we
cannot use causal inference to calculate a causal effect, even with an
infinite sample of data.

% backdoor path}

A necessary condition for unidentifiability is the presence of an
unblocked backdoor path from cause to effect. Backdoor paths are any
paths going through parents of the cause to the effect. We can block
these paths by selectively observing variables such that no
information passes through them \citep{geiger-d-sep}. If we can
observe variables along the backdoor paths such that they are blocked,
then we have satisfied the \emph{back-door criterion}
\citep{pearl2009} and we can calculate unbiased causal effects from
data.

\begin{figure} \input{figs/mutilated-cloud-aerosol.tex}
  \caption{A mutilation of Figure \ref{fig:toy}(A), where we have
    removed directed causal paths from the cause (cloud) to the effect
    (surface solar radiation). There is covariability between cloud and
    surface solar radiation that is not due to the causal connection
    between cloud and surface solar radiation, but is instead due to
    aerosol's role as a common driver.}
  \label{fig:mutilated-toy}
\end{figure}

% backdoor path with example} Understanding backdoor paths and the
backdoor criterion is helped by example. Returning to our toy example
(Figure \ref{fig:toy}A), we attempt to calculate the causal effect of
clouds on sunlight. In other words, we want to isolate the variability
of sunlight due to the causal link from cloud to sunlight (Figure
\ref{fig:toy}A). However, aerosols affect both cloud and sunlight, so
if we naively calculate a causal effect using correlations between
sunlight and cloud, we obtain a biased estimate. To demonstrate this,
consider simulated cloud, aerosol, and sunlight data from a set of
underlying equations consistent with Figure \ref{fig:toy}A and
Equation (\ref{eq:2}):


\begin{align} aerosol =& \; U_{aerosol}; \; U_{aerosol} \sim
                         \text{uniform (0, 1]}\\ cloud =& \; \text{Cloudy if } U_{cloud} +
                                                          aerosol > 1; \; U_{cloud} \sim \text{uniform (0, 1]}\\ sunlight
  =& \begin{cases} \text{Cloudy} &: 0.6 \cdot \text{downwelling clear
      sky radiation} \\ \text{Clear} &: \text{downwelling clear sky
      radiation}
  \end{cases}
                                       \label{eq:1}
\end{align}

where:

\begin{equation*} \text{downwelling clear sky radiation} =
  U_{sunlight} \cdot (1 - aerosol); \; U_{sunlight} \sim
  \text{Normal(340 W m$^{-2}$, 30 \, W m$^{-2}$)}
\end{equation*}

Now, consider not knowing the underlying generative processes, but
instead just passively observing cloud and sunlight. If one were
interested in calculating the effect of cloud on sunlight, and aerosol
data were not available, one naive approach would be to bin the data
by cloudy and clear conditions and compare the amount of sunlight
between cloudy and clear observations (Figure
\ref{fig:naive-cloud-sunlight}). This approach would suggest that
clouds reduce sunlight by, on average, 160 W m$^{-2}$; this is is a
strong overestimation of the true average effect of clouds (-68 W
m$^{-2}$), derived from Equation (\ref{eq:1}). Yet, aerosol induces
co-variability between cloud and sunlight that is unrelated to the
causal link from cloud to sunlight. Graphically, this is clarified by
removing all edges from our cause (cloud) to children of our cause (in
this case sunlight) to create a ``mutilated'' graph (Figure
\ref{fig:mutilated-toy}). By ``mutilated'', we mean that the new graph
deviates from reality, in this case by breaking all causal paths from
our cause to our effect. We see that in this mutilated graph clouds
are not independent of surface solar radiation; aerosol induces
variability in both cloud and surface solar radiation that is
unrelated to the causal path between cloud and surface solar
radiation.  However if aerosols were fixed (e.g. observed or not
varying), cloud and sunlight would be independent of each other in the
mutilated graph (Figure \ref{fig:mutilated-toy}). In other words,
conditional on aerosol, cloud and sunlight are independent in the
mutilated graph (Figure \ref{fig:mutilated-toy}), which does not
include the causal path from cloud to sunlight. And, conditional on
aerosol in the true system (Figure \ref{fig:toy}(A)), all
co-variability between cloud and sunlight is only due to the causal
edge between cloud and sunlight.  We can mathematically encode this
requirement that we must condition on aerosol to isolate the causal
effect of cloud on radiation, and doing so identifies the causal
effect of cloud on sunlight by satisfying the backdoor criterion with
\textit{adjustment} on aerosol:

\begin{figure} \includegraphics[]{naiveCloudSunlight.pdf}
  \caption{A naive approach to estimating the ``effect'' of clouds on
    sunlight: bin observations by cloudy and clear day, and compare the
    values of sunlight. This approach yields an average difference of
    160.43 W m$^{-2}$ between cloudy and clear days, and is a large
    overestimation of the true causal effect of clouds on sunlight (-68.0
    W m$^{-2}$) in these synthetic data.}
  \label{fig:naive-cloud-sunlight}
\end{figure}

\begin{equation} P(S | do(C = c)) = \int_{a} P(S \, | \, C = c, A=a)
  \, P(A=a) \; da,
  \label{eq:3}
\end{equation} where the \textit{do}-calculus \citep{pearl2009} term
($P(S \, | \, do(C\, = \,c))$) represents the probability of sunlight
if we did an experiment where we intervened and set cloud to a value
of our choosing (in this case $c$, which could be ``True'' for the
presence of a cloud, or ``False'' for no cloud). In the case that
observations of aerosols are not available (e.g., Figure
\ref{fig:toy}B, C), our causal effect is not identifiable and we
cannot use causal inference no matter how large the sample size of our
data is. The DAG is therefore a powerful analysis tool: after encoding
our domain knowledge in a causal graph, we can analyze the available
observations to determine whether a causal calculation is possible,
\textit{without needing to collect, download, or manipulate any
  data}. For more complicated graphs, causal identification can be
automated \citep[][ \url{http://www.dagitty.net/}]{shpitser2006,
  textor2017}. We later use this theory to theoretically assess which
general problems are tractable in Earth science using causal inference
(Section \ref{sec:necess-cond-caus}).

Once we have established that a causal effect is identifiable from
data, we must estimate the required observational distributions
(Equation (\ref{eq:3})) from data. Often it may be more
computationally tractable to calculate an average causal effect,
rather than the full causal distribution $P(S | do(C=c))$. Returning
to our toy example (Figure \ref{fig:toy}(A)), the average effect is
defined as:

\begin{equation} \mathbb{E}(S | do(C = c)) = \int_{s} s \, P(S = s |
  do(C=c)) \, ds,
  \label{eq:4}
\end{equation}

where $\mathbb{E}$ is the expected value. Substituting Equation
(\ref{eq:3}) into Equation (\ref{eq:4}), and rearranging gives:

\begin{equation} \mathbb{E}(S | do(C = c)) = \int_{a} P(A=a) \;
  \mathbb{E}(S \, | \, C=c, A=a) \, d a,
  \label{eq:5}
\end{equation}

Where $\mathbb{E}(S \, | \, C=c, A=a)$ is just a regression of
sunlight on cloud and aerosol. Estimating the marginal $P(A)$ is
difficult, but if we assume that our observations are independent and
identically distributed (IID) and we have a large enough sample, we
can use the law of large numbers to approximate Equation
(\ref{eq:5}). The law of large numbers states that if we observe $n$
IID samples of some process, in this case $A$ (e.g., $a_1$, $a_2$,
$\ldots$, $a_n$), then for any function $f$ \citep{shalizi2013}:

\begin{equation} \frac{1}{n} \sum_{i=1}^n f(a_i) \to \int_a P(A=a)
  f(a) \, d a.
  \label{eq:lln}
\end{equation}

In Equation (\ref{eq:5}), $c$ is fixed by the intervention, so
$\mathbb{E}(S| C=c, A=a)$ is just a function of $a$, and we can use
Equation (\ref{eq:lln}) to justify an approximation to Equation
(\ref{eq:5}) with:

\begin{equation} \mathbb{E}(S | do(C = c)) \approx \frac{1}{n}
  \sum_{i=1}^n \mathbb{E}(S \, | \, C=c, A=a_i).
  \label{eq:6}
\end{equation}

Data or prior knowledge can inform the regression function for
$\mathbb{E}(S | C=c, A=a_i)$, and as always whatever regression method
is used, it should be checked to insure it is representative of the
data. In the toy example, a linear model conditional on cloud appears
to be a good choice of regression function (Figure
\ref{fig:linear})\footnote{However note that for most real problems in
  Earth science, we probably want to use some type of non linear
  regression (neural networks could be well suited to this task).}. The
causal effect of clouds on sunlight as calculated using Equation
(\ref{eq:6})) (e.g.  $\mathbb{E}(S | do(C = \text{cloudy})) -
\mathbb{E}(S | do(C = \text{clear}))$) is -67.69 W m$^{-2}$, which
closely matches the true causal effect from Equation (\ref{eq:1}) of
-68 W m$^{-2}$. This example demonstrates how causal inference and
theory can be used to calculate unbiased average effects using
regression. Further, causal inference can be used to justify and
communicate assumptions in any observational analyses employing
regression. In the best case, the causal effect is identifiable from
the available observations, and the regression analysis can be framed
as an average causal effect. In the worst case that identification is
not possible from the available observations, one may present the
regression as observed associations between variables. However,
presentation of a causal graph still aids the reader: the reader can
see from the causal graph what the confounders and unobserved sources
of covariability are between the predictors and the output. In all
cases, the presentation of a causal graph makes explicit the
assumptions about the causal dependencies of the system. Wherever
possible, we recommend including causal graphs with any
observation-based analyses.

In summary of the main points of this introduction to causal graphical
models and \textit{do-}calculus:

\begin{itemize}
\item Graphical causal models encode our assumptions about causal
  dependencies in a system (edges are drawn \emph{from} causes \emph{to}
  effects). ``Causal dependencies'' really just refer to functional
  dependencies between inputs (causes) and outputs (effect), which are
  useful in physical sciences.
\item In order to calculate an unbiased causal effect from data, we
  must isolate the covariability between cause and effect that is due to
  the directed causal path from cause to effect. The presence of
  non-causal dependencies between the cause and effect can be deduced
  from the causal graph: the presence of an unblocked backdoor path from
  the cause to the effect leads to non-causal dependencies (and
  co-variation).
\item The backdoor criterion identifies the distributions we must
  calculate from data in order to block all backdoor paths, remove
  non-causal dependence between the cause and effect, and calculate an
  unbiased causal effect from data.
\item The \emph{average} causal effect can be reliably approximated
  with regression (Equation (\ref{eq:6})) derived from the backdoor
  criterion. In this scenario, causal theory and graphs identify the
  variables that should (and should not be) included in the regression
  in order to calculate an unbiased causal effect.
\item Causal identification is a flexible tool that provides the
  distributions that must be estimated from data, while making no
  assumptions about the forms of those distributions. However,
  parametric assumptions can be applied to make the calculation of those
  distributions from data more computationally tractable.
\end{itemize}

\begin{figure} \includegraphics[]{aerosolSunlight.pdf}
  \caption{A linear relationship between aerosol and sunlight,
    conditional on cloud. If we use linear regression to calculate the
    average causal effect of cloud on sunlight, as in Equation
    (\ref{eq:6}), our result is very close to the true causal effect of
    -68.0 W m$^{-2}$.}
  \label{fig:linear}
\end{figure}

Here we focused on the backdoor criterion to block backdoor paths. An
un-blockable backdoor path from the cause to the effect is a necessary
condition for unidentifiability. However, it is not sufficient
(e.g. there are other identification strategies like the front door
criterion and instrumental variables that do not rely on observing
variables along the backdoor path \citep{pearl2009causality}). We
focus on the backdoor criterion because it is the most fundamental and
direct method for adjusting for confounding, the most intuitive for an
introduction to causality, and is the most relevant for the generic
temporal systems present in the Earth sciences (Figure
\ref{fig:generic}, see \citet{tian2002general}).  However, causal
identification through other methods like instrumental variables and
the front door criterion can also be automated; we refer the reader to
\citet{pearl2009causality} for further discussion and software tools
like \url{http://www.dagitty.net/} and
\url{http://www.causalfusion.net} for interactive exploration.


\section{Beyond toys: causal graphs as communicators, organizers, and
  time-savers}\label{sec:causal-graphs-as}

In Section \ref{sec:causal-graphs-pearls} we used a toy example to
demonstrate a causal analysis starting with drawing graph, and ending
with the successful calculation of the average response of sunlight
(the effect) to an intervention on cloud (the cause). However, often
we may not be able to complete this last step: estimating the causal
effects from the available data, usually because there are serious
challenges due to unobserved confounding in generic Earth science
problems (Section \ref{sec:necess-cond-caus}). However, we want to
emphasize that, even if calculating a causal effect is ultimately
impossible, drawing a causal graph at the beginning of an analysis
offers tremendous benefits in terms of organization and
communication. Investing time to reason about the functional structure
of our problem at the outset can save us time in the long term,
forcing us to clarify our thinking early, expose potential challenges,
and identify intractable approaches.

Once we have drawn a causal graph, there is no cost to including in in
presentations, papers, and discussions of our results. Making our
assumptions about dependencies in the system explicit greatly improves
the interpretability and reproducibility of our results. Perhaps our
analysis and graph meet the standards for a causal interpretation, but
even if they do not, the causal graph helps the rest of the community
asses the sources of confounding in the graph that were not controlled
for, and understand if their conceptualization of the graph structure
and of the problem match the authors' assumptions. Often there are
more assumptions being made than are communicated, and even when they
are communicated, the assumptions do not always get discussed in a
precise way. Including a causal graph allows the assumptions to be
clearly known, and discussed in a precise and rigorous way
\citep{hannart-da}.

To support the idea that many analyses would benefit from a causal
graph, we will detail how a past project benefits from a causal
graph. This example also moves beyond the toy example of Section
\ref{sec:causal-graphs-pearls}, and demonstrates causal graphs'
applicability to real problems in Earth science.

\subsection{An example from our past}

In \citet{massmann2017}, the lead author of this manuscript
participated in a field campaign designed to study the impact of
microphysical rain regime (specifically the presence of ice from aloft
falling into orographic clouds) on orographic enhancement of
precipitation. This field campaign and analysis benefits from a causal
graph and is a nice real-world example argument for the more common
use of causal graphs as research tools. Our retrospective causal graph
of orographic enhancement in the Nahuelbuta mountains under steady
conditions clearly communicates our assumptions about the system
(Figure \ref{fig:ccope}).

\begin{figure} \includegraphics[]{ccope.pdf}
  \caption{A graph representing steady conditions for orographic
    enhancement during the Chilean Coastal Orographic Precipitation
    Experiment \citep[CCOPE,][]{massmann2017}. We were interested in the
    effect of ``rain regime'' on ``orographic enhancement.''  Observed
    quantities are represented by solid nodes, while unobserved quantities
    are represented by dashed nodes. All backdoor paths are blocked by
    observed quantities, so the effect of rain regime on orographic
    enhancement is identifiable.}
  \label{fig:ccope}
\end{figure}

Many of the variables in the graph are quite general
quantities. Keeping quantities general can lead to more intuitive and
interpretable graphs by limiting the number of details and nodes that
one must store in their mind. However, if logic needs clarifying, the
graph can become more explicit (e.g., differentiating wind into speed,
direction, and spatial distribution, both horizontally and
vertically). As graphs become more complicated, one can leverage
interactive visualization software, or use static graph abstractions
like plates \citep{bishop2006pattern}, which are particularly well
suited to representing repeated structure common to spatiotemporal
systems.

While the exact details of the graph (e.g., ``wind'', ``stability'',
and ``atmospheric moisture'' all refer to upwind conditions and we
assume that these upwind conditions are the relevant ``boundary
condition'' for the downwind orographic clouds and precipitation) are
not as important for this example case, what is important is that the
field campaign's effect of interest, rain regime on orographic
enhancement, is identifiable from the field campaign's
observations. This is subject to the assumptions encoded in the graph,
but those assumptions are explicitly represented and communicated with
the graph. The causal graph helps interpret the field campaign's
results, and in some sense proves that the design of the field
campaign is sound.

For field campaigns, causal graphs are particularly useful at the
planning and proposal stage. Such a causal graph could be included in
any field campaign proposal, improving communication about the system
and also rigorously justifying the campaign’s observations as
necessary for calculating the desired effect(s). Even before the
proposal, one could start with a causal graph, and then analyze the
causal graph to determine which observations are needed to meet the
campaign’s goals. Building on this idea, one could attach costs
associated with observing each variable in the graph, and
automatically determine the set of observations that minimizes cost
while still allowing us to calculate our effect(s) of interest. This
analysis communicates to proposal reviewers the campaign is rigorously
optimized to minimize cost while still tractably calculating the
causal effect(s) of interest.

While this is just one example, it demonstrates that causal graphs are
useful beyond toy problems in the Earth system. Additionally, as we
will see in Section \ref{sec:necess-cond-caus}, we can draw quite
general graphs that are representative of many problems in Earth
science. We hope the reader considers drawing a causal graph as a
first step in their next project; they help structure, organize, and
clarify our analysis and its assumptions.


\section{Overcoming unobserved confounding: tractable classes of
  problems in generic Earth systems}
\label{sec:necess-cond-caus}

So far we have focused on toy (Section \ref{sec:causal-graphs-pearls})
and very specific (Section \ref{sec:causal-graphs-as}) examples. We
now turn our attention to more general and generic problems in Earth
science systems, the common challenges we may encounter when
attempting causal inference, and how we can overcome these challenges.

Earth science systems are (typically) dynamical systems evolving
through time according to an underlying system state
\citep{lorenz-1963,lorenz1996predictability,majda-state}. This offers
both advantages and challenges for causal inference. When constructing
causal graphs we benefit from the temporal ordering of events: we know
that future events can have no causal effect on the past. However,
confounding due to incomplete observation of the system's state space
introduces challenges.\footnote{Note that incomplete observation of
  the system precludes many ``causal discovery'' algorithms as well
  \citep[see ][ for a great review]{runge2019inferring}. Despite this
  paper's different causal lens, many of these same issues must be
  overcome in causal discovery as well.}

Causal identification and tractable causal inference in Earth science
requires assumptions about the unobserved portions of the state
space. Without such assumptions the unobserved portions of the state
space will introduce confounding for any causal affect of interest
(Figure \ref{fig:generic}a). For example, we generally do not observe
the state space at every time (e.g. $S(t-1/2)$, Figure
\ref{fig:generic}a), and at any given time, we do not observe the
state space at all locations and for all state variables (e.g. $S(t)$
and $S(t-1)$ in Figure \ref{fig:generic}a). So, if we are interested
in the causal effect of any state variable at time $t$ on some
variable at time $t+1$ (e.g., $E$ in Figure \ref{fig:generic}a), then
the causal effect will be confounded by the unobserved portions of the
state space, and calculating a causal effect is impossible
(un-identifiable) without additional assumptions. We will apply causal
graph theory to identify the additional assumptions that would make
the calculations of causal effects tractable, and may be reasonable in
many Earth science situations.

\begin{figure} \input{figs/generic-graph.tex}
  \caption{Generic graphs of the Earth system state sequence, limited
    to a 3 time-step sequence subset of the infinite
    sequence. \textbf{A):} The reality of the observed Earth system:
    unobserved nodes are outlined by dashed lines. We only observe the
    state space at certain times (e.g., no observations at $S(t-1/2)$). At
    times with observations, we only partially observe the full state
    ($S(t)$, $S(t-1)$). In the scenario that we are interested in
    calculating the causal effect of any portion of the state space at
    time $t$ on some effect ($E$) at time $t+1$, the causal effect will be
    confounded by the unobserved portions of the state space, and
    calculating the causal effect is impossible (un-identifiable) without
    additional assumptions. \textbf{B):} The reduction of the Earth system
    graph under an assumption that we can reconstruct the state $S(t)$
    from the observable portions of the state space at lagged times $< t$
    (see Section \ref{sec:stat-reconstr-state}). \textbf{C):} The
    reduction of the Earth system graph under an assumption that missing
    temporal observations (e.g. $S(t-1/2)$ in \textbf{A)}) induce
    independent random variations in the cause ($C(t)$) and effect
    ($E(t+1)$). $S'(t)$ denotes the state space, not including the cause
    $C(t)$ (see Sections
    \ref{sec:miss-temp-observ},\ref{sec:observ-port-state}).}
  \label{fig:generic}
\end{figure}

\subsection{Statistical reconstruction of the state space with time
  lagged observations}
\label{sec:stat-reconstr-state}

Takens' theorem implies that we can reconstruct the unobserved
portions of the state space using lagged observations back in time
\citep{takens1981detecting,deyle2011generalized,Sugihara496}. In this
case, the causal graph is greatly simplified (Figure
\ref{fig:generic}b). If we assume that the state is reconstructable,
then we can calculate the causal effect of any reconstructed state on
any future variable or process. The primary advantages of this
approach is that it is the minimum assumption required to calculate a
causal effect in the generic Earth system graph proposed in Figure
\ref{fig:generic}a (i.e., we do not need to assume anything about the
relationship between the unobserved portions of the state space and
the effect). However this simplification comes at a cost: we can only
examine the effect of different global states on future variables, and
we cannot examine the causal effect of a specific variable
(e.g. $C(t)$) on another (e.g. $E(t+1)$), because the mapping from
individual observations to a state space reconstruction transforms the
individual observation's importance to additionally include the
unknowable and unobserved portions of the state space. So, an
intervention on any individual observation is of ambiguous meaning
(e.g. we do not know what are the unobserved variables we are
intervening on). Instead, we limit ourselves to only examining the
causal effect of changes in the entire state
holistically. Additionally, this approach requires a shift in
interpretation from the relatively straightforward interventions on
observable variables we have considered thus far to one where we
intervene on the entire state.

For example, consider that we reconstructed our state space from
available observations, and this reconstruction reduced to a discrete
state space of $N$ states, with associated patterns in the
observations at time $\leq t$. If we are interested in the effect of a
change in state on a process at time $t+1$, for example a change from
state $1$ to state $2$, this corresponds to an intervention on the
\textit{entire state space at time $t$}. This includes the unobserved
portions of the state space. So, the intervention no longer consists
of just intervening on the observed variables to change them from
observations consistent with state $1$ to state $2$, but instead
intervening on the observed variables \emph{and all unobserved
  variables consistent with the system and changes from state $1$ to
  $2$.}  Conceptualizing unknowable changes in unobserved variables
represents a significant barrier to interpretation, and motivates the
exploration of stronger assumptions that result in a clearer
interpretation (see Section \ref{sec:miss-temp-observ},
\ref{sec:observ-port-state}, \ref{human}).

In practice, we might not know the number of lagged observations that
are required for reconstruction of the state space at time $t$. One
approach is to use the data for guidance. This involves reconstructing
the state at time $t$ with iteratively increasing numbers of lags. We
have confidence we have reproduced the state when the observations at
time $t$ are conditionally independent of each other given the state
reconstruction. However, this approach potentially introduces even
more problems, including but not limited to the assumptions required
in our conditional independence tests. This example highlights the
challenges associated with any method relying on state space
reconstruction with lagged observations: to be sure we reconstruct the
state space we must include observations from a potentially infinite
temporal extent, and there might not be any reliable way to check that
we successfully reconstructed the state space.

\subsection{Missing temporal observations induce independent random
  variations in cause and effect}
\label{sec:miss-temp-observ}

If we can assume that the missing temporal observations
(e.g. $S(t-1/2)$ in Figure \ref{fig:generic}a) induce independent
random variations in the cause and effect, conditional on $S(t-1)$,
then the causal graph reduces to Figure \ref{fig:generic}c. In this
case, if we can reconstruct the state space at time $t-1$ using lagged
observations at time t $\leq t-1$, then we can block all backdoor
paths between our cause $C(t)$ and any future effect $E(t+1)$. In this
case, the causal effect would be calculated as:

\begin{equation} P(E(t+1)| do(C(t)=c)) = \int_{S(t-1)} P(E(t+1) \, |
  \, C(t)=c, S(t-1) = s )\; P(S(t-1)=s) \, d s,
\end{equation}

where $S(t-1)$ is reconstructed from time lags of observations at
times $\leq t-1$. From the graph it may appear that we could also
block backdoor paths by conditioning on $S'(t)$. However, under these
assumptions $S'(t)$ is incalculable because if we attempt to
reconstruct $S'(t)$ using lagged observations of the state space, the
reconstruction will estimate $S$ rather than $S'$, which includes
$C(t)$ because $C$ is a part of the state space. So, a portion of the
cause's role in the system's evolution will be falsely incorporated
into the reconstruction of the state space, and our causal effect will
be biased.

The assumption that missing temporal observations induce independent
random variations in the cause and effect is required because
otherwise there would be an open backdoor path between the cause and
effect through the unobserved time slice (e.g. at time $t-1/2$ in
Figure \ref{fig:generic}a). Relative to Section
\ref{sec:stat-reconstr-state}, the advantage of this approach to
causal inference is that we can calculate the causal effects of
individual observations (e.g, $C(t)$ on $E(t+1)$), rather than needing
to interpret the causal impact of the entire state holistically (e.g.,
$S(t)$ on $E(t+1)$). However, this approach requires an additional
assumption that the missing temporal observations do not induce
dependencies between the cause and effect of interest. As with Section
\ref{sec:stat-reconstr-state}, we must be able to reconstruct the
state space, which may not always be possible, and may also confuse
interpretation.


\subsection{The unobserved portion of the state space does not affect
  the effect}
\label{sec:observ-port-state}

If we assume that we observe all portions of the state space that
affect the effect, then we can calculate the effect of any state
variable on future processes. In this case, the graph is as in Figure
\ref{fig:generic}c, but $S'(t)$ corresponds to all observations at
time $t$ not including $C$, and so can be used to block all backdoor
paths. If we can assume that there are no interactions between
observations at time $t$ then we can calculate the causal effect as:

\begin{equation} P(E(t+1)| do(C(t)=c)) = \int_{S'(t)} P(E(t+1) \, | \,
  C(t)=c, S'(t) = s )\; P(S'(t-1)=s) \, d \, s,
\end{equation}

where $S'(t)$ are all observations at time $t$, not including the
cause $C(t)$. However, when blocking backdoor paths with simultaneous
observations it is important to consider whether there are
interactions between observations at time $t$, and whether the
observations are truly simultaneous. Whether there can be interactions
between observations is a function of the temporal and spatial
extent/averaging of the observations. If observations are
instantaneous point observations indexed in time, then interactions
between them can likely be ignored. However, often the spatial and/or
temporal extents of observations overlap, and in this case an
assumption of zero interactions between observations is more difficult
to justified. Yet, in this case, we can still block backdoor paths by
conditioning on past observations ($S(t-1)$):

\begin{equation} P(E(t+1)| do(C(t)=c)) = \int_{S(t-1)} P(E(t+1) \, |
  \, C(t)=c, S(t-1) = s )\; P(S(t-1)=s) \, d \, s.
\end{equation}

Importantly, the assumption that we observe all variables relevant to
an effect is a strong assumption. However, it provides significant
benefits both in terms of the statistical complexity of estimating the
causal effect, as well as in interpretation of the causal effect. We
do not need to reconstruct the state space using lagged observations,
which can be a statistically and computationally challenging
problem. Additionally, the assumption that we observe everything
relevant to the effect is much easier to interpret relative to an
assumption about the degree to which we can (or cannot) reconstruct
the relevant state space with lagged observations.

\subsection{Applications at the human-Earth system interface: when the
  cause is approximately independent of the system.}
\label{human}

Causal inference becomes substantially more tractable under an
assumption that our causes of interest are independent from the
evolution of the state space (Figure \ref{fig:forcing}). While such a
strong assumption may seem unjustifiable due to many feedbacks within
the Earth system, recent historical examples suggest that this
assumption may be applicable to some situations. For example,
coordinated global human response to global warming is relatively
independent of the climate state \citep{arto2014drivers}. We have
failed to reduce green house gas emissions even as global temperature
increased. Instances of reduced rises in global green house gas
emissions are usually due to global economic recession (e.g. the 2008
financial crisis) or pandemic (COVID-19 crisis) rather than factors
directly tied to the climate state. In these examples many global
social, political, health, and economic factors are the primary causes
of global green house gas emission, and while the climate system may
affect these factors, the historical evidence suggests that the
climate system exerts a relatively small impact
\citep{arto2014drivers}.

\begin{figure} \includegraphics[]{forcing-graph.pdf}
  \caption{A generic graph asserting an assumption that there are
    forcing external to the evolution of the state-space}
  \label{fig:forcing}
\end{figure}

Generally this logic applies to many systems on the "human-climate"
interface, such as land-use land-cover change in urban centers where
urban planning is relatively independent of recent climate
history. The general graph in Figure \ref{fig:forcing} is applicable
to any analyses for which: 1) we wish to examine the effect of human
behavior on the environment, and 2) we can assume human behavior is
approximately independent of the climate state. Because the climate
state space does not affect the cause (human behavior), there are no
unblocked backdoor paths through the unobserved portions of the state
space. Causal inference is particularly tractable for this class of
problems.

However, care is still required at the human-climate interface:
human-based sources of confounding may need to be controlled for
depending on the cause of interest. For example, CO$_2$ emissions
decreased during the COVID-19 crisis, but a lot of other human
behaviors also changed, and these changes must be controlled for
(e.g., the COVID-19 virus is a common cause to many large changes in
human behavior in 2020). This example highlights the importance of
drawing a bespoke graph for every application and rigorously reasoning
about the causal structure.

\section{Conclusions}

In summary, this review showed that:

\begin{itemize}
\item Causal graphs concisely and clearly encode assumptions about
  causal/functional dependencies between processes. Including a causal
  graph benefits any observational or modeling analysis, including those
  that use regression. Depending on the observations available and the
  causal structure of the system, regression analyses can be interpreted
  as an average, generalized, functional mapping from causes to
  effects. This causal approach opens up a new path to calculate
  functional relationships when we either do not know the functional
  form a priori, or it is too computationally intractable to calculate
  from models.
\item Whether or not a causal effect can be calculated from data is
  determined exclusively by the causal graph. Thus the tractability of a
  causal analyses, or the strength of assumptions necessary to make an
  analysis tractable, is determined and assessed before collecting,
  generating, or manipulating data (which can cost a tremendous amount
  in terms of researchers' time or computational resources). We
  recommend early causal analyses to determine tractability during a
  project's conception, before resources are spent obtaining or
  analyzing data.
\item Because the Earth system evolves as a dynamical system through
  time, we can construct broadly applicable, generic Earth system
  causal graphs. However, causal inference in Earth science also
  presents challenges: we only partially observe the state space of
  the system.
\item These challenges can be alleviated by applying causal theory to
  generic causal graphs of the Earth system and identifying the
  assumptions that allow for causal inference from data. These are
  assumptions that:
  \begin{itemize}
  \item The state space of the system is reconstructable from lagged
    observations of the system, as allowed by Takens' theorem (Section
    \ref{sec:stat-reconstr-state}, Section \ref{sec:miss-temp-observ}), or
  \item The effect of interest is only causally affected by the
    observed portion of the state space (Section
    \ref{sec:observ-port-state}), or
  \item The cause of interest can be assumed to be independent of the
    evolution of the system's state (e.g. forcing) (Section \ref{human}).
  \end{itemize}
\end{itemize}

Here we focus on the fundamentals of calculating causal effects from
data. However, causal inference is a thriving active area of research,
and there are many other causal inference techniques and abstractions
that could benefit the Earth system research community. For example,
there are techniques for representing variables observed under
selection bias in the causal graph and analyzing whether a causal
effect can be calculated (i.e. identified) given selection bias
\citep[e.g.,][]{bareinboim2014recovering}. Selection bias is very
relevant in Earth science. For example, satellite observations are
almost always collected under selection bias (e.g. they sample at
certain local times of the day, clouds obscure surface data,
etc.). Additionally, transportability
\citep[e.g.,][]{bareinboim2012transportability} identifies whether one
can calculate a causal effect in a passively observed target domain,
by merging experiments from source domains that may differ from the
target domain. A potential application for transportability in earth
sciences would be to merge numerical model experiments (e.g., global
climate models, cloud models, etc.) and formally transport their
results to the real world. In this case, numerical models are the
source domains that differ from the target domain (``real world'') due
to approximations.

However, many of these developments in causal theory are not yet well
established in applied domains. We believe that the use of causal
graphs to organize and structure analyses is mature and directly
applicable to many applications, and can serve as a gateway to these
more sophisticated (and less mature) causal methods. We hope that
drawing and including causal graphs in Earth science research becomes
much more common, and that this manuscript provides some of the the
necessary foundation for readers to feel comfortable using causal
graphs in their next project.

\paragraph{Acknowledgments} The authors want to than Beth Tellman,
James Doss-Gollin, David Farnham, and Masa Haraguchi for thoughtful
feedback and comments that greatly improved an earlier version of this
manuscript.


\bibliography{references.bib}

\appendix
\section{Basic probability and syntax}
\label{prob-theory}

In this paper we use capital letters to represent random variables
(e.g., ``$X$''). For example, $P(X)$ is the probability distribution
of a random variable $X$. $P(X)$ is a function of one variable that
outputs a probability (or density, in the case of continuous
variables) given a specific value for $X$. We represent specific
values that a random variable can take with lowercase letters (e.g.,
$x$ in the case of $X$). $P(X)$ is shorthand; a more descriptive but
less concise way to write $P(X)$ is $P(X=x)$ which represents the fact
that $P(X)$ is a function of a specific value of $X$, represented by
$x$. We use both notations, and $P(X)$ has the same meaning as
$P(X=x)$.

For the unfamiliar reader, there are a few basic rules and definitions
in probability that provide relatively complete foundations for
building deeper understanding of probability. These are the
\textbf{sum rule}:

\begin{equation} P(X=x) = \sum_Y P(X=x,\, Y=y)
  \label{eq:sum}
\end{equation}

and the \textbf{product rule}:

\begin{equation} P(X=x, \, Y=y) = P(X = x \, | \, Y=y ) P(Y=y) = P(Y =
  y \, | \, X=x ) P(X=x)
  \label{eq:product}
\end{equation}

The \textit{joint probability distribution} ($P(X=x,Y=y)$) is the
probability that the random variable $X$ equals some value $x$
\emph{and} the random variable $Y$ equals $y$. The joint distribution
is a function of two variables, $x$ and $y$ which are values in the
domains of the random variables $X$ and $Y$ respectively. The
\textit{conditional probability distribution} ($p(X = x \, | \, Y=y
)$) is also a function of two variables $x$ and $y$, but it is the
probability of observing $X$ equal to $x$, given that we have observed
$Y$ equal to $y$. In other words, if we filter our domain to only
values where $Y=y$, then $p(X = x \, | \, Y=y )$ is the probability of
observing $X=x$ in this sub-domain where $Y=y$. The \textit{marginal
  probability distribution} ($P(Y=y)$) is just the probability that $Y$
equals some value $y$, and is a function of only $y$. We can calculate
the marginal probability from the joint distribution by summing over
all possible values values of the other random variables in the joint
(the ``sum rule'' - Equation (\ref{eq:sum})). Additionally, the joint
distribution can factorize into a product of conditional and marginal
distributions (``the product rule'' - Equation
(\ref{eq:product})). These two simple rules can be used to build much
of the theory and applications of probability theory (e.g., Bayes'
theorem $P(Y|X) =\frac{P(X|Y) P(Y)}{P(X)}$). While Equations
(\ref{eq:sum}) deals with probability distributions of discrete random
variables, there is also a sum rule analog for continuous random
variables and probability density functions (the syntax of the product
rule is the same):

\begin{equation*} P(X=x) = \int_Y P(X=x,\, Y=y) \, dy
\end{equation*}

where $\int_{Y}$ represents an integral over the domain of $Y$ (e.g.,
$\int_{-\infty}^{\infty}$ if $Y$ is a Gaussian random variable).


\end{document}